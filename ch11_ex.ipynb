{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\\nNo, doing this will prevent the network from breaking symmetry. Each unit gets a signal\\nequal to the sum of inputs and outputs sigmoid(sum(inputs)). If all weights are zero then\\nall hidden units will get zero signal. So if all weights are the same, then all units\\nin the hidden layer will be the same too no matter the input. Simply, if all\\nunits start with the same wieght, then they will follow the same gradient which could\\nslow down learning if there are many local minima.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "No, doing this will prevent the network from breaking symmetry. Each unit gets a signal\n",
    "equal to the sum of inputs and outputs sigmoid(sum(inputs)). If all weights are zero then\n",
    "all hidden units will get zero signal. So if all weights are the same, then all units\n",
    "in the hidden layer will be the same too no matter the input. Simply, if all\n",
    "units start with the same wieght, then they will follow the same gradient which could\n",
    "slow down learning if there are many local minima.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2. Is it okay to initialize the bias terms to 0?\\nYes, because symmetry breaking is done by initializing weights as random values.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2. Is it okay to initialize the bias terms to 0?\n",
    "Yes, because symmetry breaking is done by initializing weights as random values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n3. Name three advantages of the ELU activation function over ReLU.\\n    a. ELU won't saturate (gradient = 0) when z < 0, so units won't die\\n    b. The function is smooth everywhere including z = 0 which helps gradient\\n    descent because it does not bounce as much lect and right of z=0.\\n    c. ELU takes on negative values when z < 0, which allows the unit to have an average output closer to 0. \\n    This helps alleviate the vanishing gradients problem, as discussed earlier.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "3. Name three advantages of the ELU activation function over ReLU.\n",
    "    a. ELU won't saturate (gradient = 0) when z < 0, so units won't die\n",
    "    b. The function is smooth everywhere including z = 0 which helps gradient\n",
    "    descent because it does not bounce as much lect and right of z=0.\n",
    "    c. ELU takes on negative values when z < 0, which allows the unit to have an average output closer to 0. \n",
    "    This helps alleviate the vanishing gradients problem, as discussed earlier.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4. In which cases would you want to use each of the following activation functions: \\nELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\\nELU:\\nleaky ReLU:\\nReLU\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4. In which cases would you want to use each of the following activation functions: \n",
    "ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "ELU: Generally the best to use, but slower to compute than ReLU and its variants.\n",
    "leaky ReLU: Use if you care a lot about runtime performance. Preferred over ReLU\n",
    "on large datasets, but might overfit on smaller datasets.\n",
    "ReLU: More simple than leaky ReLU, but in general leaky and ELU are better.\n",
    "tanh: Quicker convergence then logstic functions. \n",
    "Can be useful in the output layer if you need to output a number between â€“1 and 1, \n",
    "but nowadays it is not used much in hidden layers. \n",
    "logistic: Useful if you want to estimate a probability, but is rarely used in hidden layers.\n",
    "softmax: Useful in output layer to output mulually exclusive classes (MNIST labels).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
