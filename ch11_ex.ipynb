{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\\nNo, doing this will prevent the network from breaking symmetry. Each unit gets a signal\\nequal to the sum of inputs and outputs sigmoid(sum(inputs)). If all weights are zero then\\nall hidden units will get zero signal. So if all weights are the same, then all units\\nin the hidden layer will be the same too no matter the input. Simply, if all\\nunits start with the same wieght, then they will follow the same gradient which could severely\\nslow down learning if there are many local minima.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "No, doing this will prevent the network from breaking symmetry. Each unit gets a signal\n",
    "equal to the sum of inputs and outputs sigmoid(sum(inputs)). If all weights are zero then\n",
    "all hidden units will get zero signal. So if all weights are the same, then all units\n",
    "in the hidden layer will be the same too no matter the input. Simply, if all\n",
    "units start with the same wieght, then they will follow the same gradient which could severely\n",
    "slow down learning if there are many local minima.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2. Is it okay to initialize the bias terms to 0?\\nYes, because symmetry breaking is done by initializing weights as random values.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2. Is it okay to initialize the bias terms to 0?\n",
    "Yes, because symmetry breaking is done by initializing weights as random values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n3. Name three advantages of the ELU activation function over ReLU.\\n    a. ELU won't saturate (gradient = 0) when z < 0, so units won't die\\n    b. The function is smooth everywhere including z = 0 which helps gradient\\n    descent because it does not bounce as much lect and right of z=0.\\n    c. ELU takes on negative values when z < 0, which allows the unit to have an average output closer to 0. \\n    This helps alleviate the vanishing gradients problem, as discussed earlier.\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "3. Name three advantages of the ELU activation function over ReLU.\n",
    "    a. ELU won't saturate (gradient = 0) when z < 0, so units won't die\n",
    "    b. The function is smooth everywhere including z = 0 which helps gradient\n",
    "    descent because it does not bounce as much lect and right of z=0.\n",
    "    c. ELU takes on negative values when z < 0, which allows the unit to have an average output closer to 0. \n",
    "    This helps alleviate the vanishing gradients problem, as discussed earlier.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4. In which cases would you want to use each of the following activation functions: \\nELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\\nELU: Generally the best to use, but slower to compute than ReLU and its variants.\\nleaky ReLU: Use if you care a lot about runtime performance. Preferred over ReLU\\non large datasets, but might overfit on smaller datasets.\\nReLU: More simple than leaky ReLU, but in general leaky and ELU are better. ReLU\\nis able to output zero in some cases which can be useful.\\ntanh: Quicker convergence then logstic functions. \\nCan also be useful in the output layer if you need to output a number between –1 and 1, \\nbut nowadays it is not used much in hidden layers. \\nlogistic: Useful if you want to estimate a probability, but is rarely used in hidden layers.\\nsoftmax: Useful in output layer to output mulually exclusive classes (MNIST labels).\\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4. In which cases would you want to use each of the following activation functions: \n",
    "ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "ELU: Generally the best to use, but slower to compute than ReLU and its variants.\n",
    "leaky ReLU: Use if you care a lot about runtime performance. Preferred over ReLU\n",
    "on large datasets, but might overfit on smaller datasets.\n",
    "ReLU: More simple than leaky ReLU, but in general leaky and ELU are better. ReLU\n",
    "is able to output zero in some cases which can be useful.\n",
    "tanh: Quicker convergence then logstic functions. \n",
    "Can also be useful in the output layer if you need to output a number between –1 and 1, \n",
    "but nowadays it is not used much in hidden layers. \n",
    "logistic: Useful if you want to estimate a probability, but is rarely used in hidden layers.\n",
    "softmax: Useful in output layer to output mulually exclusive classes (MNIST labels).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\\nIf the momentum hyperparameter is too close to 1 then it will make the optimizer\\noscillate (overshoot a bit, then come back, overshoot again) many times before stablizing at the minimum.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
    "If the momentum hyperparameter is too close to 1 then it will make the optimizer\n",
    "oscillate (overshoot a bit, then come back, overshoot again) many times before stablizing at the minimum.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n6. Name three ways you can produce a sparse model.\\n    a. Train the model as usual, then get rid of the tiny weights (set them to 0).\\n    b. Use ℓ1 regularization during training, which pushes the optimizer toward sparsity\\n    c. Apply Dual Averaging, often called Follow The Regularized Leader (FTRL). When used with ℓ1 regularization, this technique often leads to very sparse models. \\n    TensorFlow implements a variant of FTRL called FTRL-Proximal18 in the FTRLOptimizer class.\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "6. Name three ways you can produce a sparse model.\n",
    "    a. Train the model as usual, then get rid of the tiny weights (set them to 0).\n",
    "    b. Use ℓ1 regularization during training, which pushes the optimizer toward sparsity\n",
    "    c. Apply Dual Averaging, often called Follow The Regularized Leader (FTRL). When used with ℓ1 regularization, this technique often leads to very sparse models. \n",
    "    TensorFlow implements a variant of FTRL called FTRL-Proximal18 in the FTRLOptimizer class.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\\nYes, dropout tends to slow down convergence, but usually results in a better model. \\nNo, it does't slow down inference because dropout is only done during training.\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
    "Yes, dropout tends to slow down convergence, but usually results in a better model. \n",
    "No, it does't slow down inference because dropout is only done during training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n8. Deep Learning.\\n\\n    a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\\n\\n    b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\\n\\n    c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\\n\\n    d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\\n\\n    e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "8. Deep Learning.\n",
    "\n",
    "    a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "\n",
    "    b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "\n",
    "    c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "\n",
    "    d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n",
    "\n",
    "    e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from functools import partial\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 100\n",
    "n_hidden5 = 100\n",
    "n_outputs = 5\n",
    "\n",
    "def dnn(X, y, initializer=None, learning_rate=0.001, training=True):\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            #batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "            #              training=training, momentum=0.9)\n",
    "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                         kernel_initializer=he_init, activation=tf.nn.elu)\n",
    "            #bn1 = batch_norm_layer(hidden1)\n",
    "            #bn1_act = tf.nn.elu(bn1)\n",
    "            hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                         kernel_initializer=he_init, activation=tf.nn.elu)\n",
    "            #bn2 = batch_norm_layer(hidden2)\n",
    "            #bn2_act = tf.nn.elu(bn2)\n",
    "            hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\",\n",
    "                         kernel_initializer=he_init, activation=tf.nn.elu)\n",
    "            #bn3 = batch_norm_layer(hidden3)\n",
    "            #bn3_act = tf.nn.elu(bn3)\n",
    "            hidden4 = tf.layers.dense(hidden3, n_hidden4, name=\"hidden4\",\n",
    "                         kernel_initializer=he_init, activation=tf.nn.elu)\n",
    "            #bn4 = batch_norm_layer(hidden4)\n",
    "            #bn4_act = tf.nn.elu(bn4)\n",
    "            hidden5 = tf.layers.dense(hidden4, n_hidden5, name=\"hidden5\",\n",
    "                         kernel_initializer=he_init, activation=tf.nn.elu)\n",
    "            #bn5 = batch_norm_layer(hidden5)\n",
    "            #bn5_act = tf.nn.elu(bn5)\n",
    "            \n",
    "            logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "            #logits = batch_norm_layer(logits_before_bn, name=\"bn_outputs\")\n",
    "            \n",
    "            Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return accuracy, training_op, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir = log_dir(\"dnn\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 784), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "#training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "accuracy, training_op, init, saver = dnn(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 1.0 Val accuracy: 0.985927\n",
      "Epoch: 0 \tAcc: 0.985927\n",
      "1 Train accuracy: 1.0 Val accuracy: 0.98319\n",
      "2 Train accuracy: 1.0 Val accuracy: 0.986708\n",
      "3 Train accuracy: 1.0 Val accuracy: 0.985927\n",
      "4 Train accuracy: 1.0 Val accuracy: 0.990227\n",
      "5 Train accuracy: 1.0 Val accuracy: 0.992963\n",
      "Epoch: 5 \tAcc: 0.992963\n",
      "6 Train accuracy: 0.98 Val accuracy: 0.99179\n",
      "7 Train accuracy: 1.0 Val accuracy: 0.991009\n",
      "8 Train accuracy: 1.0 Val accuracy: 0.989836\n",
      "9 Train accuracy: 1.0 Val accuracy: 0.992572\n",
      "10 Train accuracy: 1.0 Val accuracy: 0.992572\n",
      "Epoch: 10 \tAcc: 0.992572\n",
      "restoring the best model\n",
      "INFO:tensorflow:Restoring parameters from ./my_ch11_dnn_model\n",
      "Test accuracy: 0.994551\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "n_epochs = 151\n",
    "batch_size = 50\n",
    "\n",
    "checkpoint_path = \"/tmp/my_ch11_dnn_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_ch11_dnn_model\"\n",
    "\n",
    "acc_best = 0\n",
    "\n",
    "#extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        \n",
    "    # get training samples where y < 5\n",
    "    X_train, y_train = mnist.train.next_batch(mnist.train.num_examples)\n",
    "    clean_indices = np.where(y_train < 5)\n",
    "    X_train_clean = X_train[clean_indices]\n",
    "    y_train_clean = y_train[clean_indices]\n",
    "    \n",
    "    # get validation samples where y < 5\n",
    "    val_clean_indices = np.where(mnist.validation.labels < 5)\n",
    "    X_val_clean = mnist.validation.images[val_clean_indices]\n",
    "    y_val_clean = mnist.validation.labels[val_clean_indices]\n",
    "    \n",
    "    # get test samples where y < 5\n",
    "    test_clean_indices = np.where(mnist.test.labels < 5)\n",
    "    X_test_clean = mnist.test.images[test_clean_indices]\n",
    "    y_test_clean = mnist.test.labels[test_clean_indices]\n",
    "    \n",
    "    def next_batch(batch_size):\n",
    "        random = np.random.choice(len(X_train_clean), batch_size)\n",
    "        return X_train_clean[random], y_train_clean[random]\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = next_batch(batch_size)\n",
    "            sess.run(training_op,\n",
    "                     feed_dict={ X: X_batch, y: y_batch})\n",
    "        #loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_val_clean,\n",
    "                                           y: y_val_clean})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "        #file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tAcc:\", acc_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            if acc_val > acc_best:\n",
    "                acc_best = acc_val\n",
    "                saver.save(sess, final_model_path)\n",
    "            # early stopping\n",
    "            else:\n",
    "                break\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "                \n",
    "# restore best model\n",
    "with tf.Session() as sess:\n",
    "    print('restoring the best model')\n",
    "    saver.restore(sess, final_model_path)\n",
    "    acc_test_val = accuracy.eval(feed_dict={X: X_test_clean, y: y_test_clean})\n",
    "    print(\"Test accuracy:\", acc_test_val)\n",
    "\n",
    "os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n9. Transfer learning.\\n\\n    a. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one.\\n\\n    b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?\\n\\n    c. Try caching the frozen layers, and train the model again: how much faster is it now?\\n\\n    d. Try again reusing just four hidden layers instead of five. Can you achieve a higher precision?\\n\\n    e. Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "9. Transfer learning.\n",
    "\n",
    "    a. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one.\n",
    "\n",
    "    b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "\n",
    "    c. Try caching the frozen layers, and train the model again: how much faster is it now?\n",
    "\n",
    "    d. Try again reusing just four hidden layers instead of five. Can you achieve a higher precision?\n",
    "\n",
    "    e. Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outputs/kernel/Initializer/random_uniform/shape', 'outputs/kernel/Initializer/random_uniform/min', 'outputs/kernel/Initializer/random_uniform/max', 'outputs/kernel/Initializer/random_uniform/RandomUniform', 'outputs/kernel/Initializer/random_uniform/sub', 'outputs/kernel/Initializer/random_uniform/mul', 'outputs/kernel/Initializer/random_uniform', 'outputs/kernel', 'outputs/kernel/Assign', 'outputs/kernel/read', 'outputs/bias/Initializer/zeros', 'outputs/bias', 'outputs/bias/Assign', 'outputs/bias/read', 'dnn/model/outputs/MatMul', 'dnn/model/outputs/BiasAdd', 'dnn/train/gradients/dnn/model/outputs/BiasAdd_grad/BiasAddGrad', 'dnn/train/gradients/dnn/model/outputs/BiasAdd_grad/tuple/group_deps', 'dnn/train/gradients/dnn/model/outputs/BiasAdd_grad/tuple/control_dependency', 'dnn/train/gradients/dnn/model/outputs/BiasAdd_grad/tuple/control_dependency_1', 'dnn/train/gradients/dnn/model/outputs/MatMul_grad/MatMul', 'dnn/train/gradients/dnn/model/outputs/MatMul_grad/MatMul_1', 'dnn/train/gradients/dnn/model/outputs/MatMul_grad/tuple/group_deps', 'dnn/train/gradients/dnn/model/outputs/MatMul_grad/tuple/control_dependency', 'dnn/train/gradients/dnn/model/outputs/MatMul_grad/tuple/control_dependency_1', 'outputs/kernel/Adam/Initializer/zeros', 'outputs/kernel/Adam', 'outputs/kernel/Adam/Assign', 'outputs/kernel/Adam/read', 'outputs/kernel/Adam_1/Initializer/zeros', 'outputs/kernel/Adam_1', 'outputs/kernel/Adam_1/Assign', 'outputs/kernel/Adam_1/read', 'outputs/bias/Adam/Initializer/zeros', 'outputs/bias/Adam', 'outputs/bias/Adam/Assign', 'outputs/bias/Adam/read', 'outputs/bias/Adam_1/Initializer/zeros', 'outputs/bias/Adam_1', 'outputs/bias/Adam_1/Assign', 'outputs/bias/Adam_1/read', 'dnn/train/Adam/update_outputs/kernel/ApplyAdam', 'dnn/train/Adam/update_outputs/bias/ApplyAdam']\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_ch11_dnn_model.meta\")\n",
    "graph = tf.get_default_graph()\n",
    "#print([op.name for op in sess.graph.get_operations() if 'bn_outputs' in op.name and 'read' in op.name])\n",
    "print([n.name for n in tf.get_default_graph().as_graph_def().node if 'output' in n.name])\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "#training = tf.get_default_graph().get_tensor_by_name(\"training:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"dnn/loss/loss:0\")\n",
    "Y_proba = tf.get_default_graph().get_tensor_by_name(\"dnn/model/Y_proba:0\")\n",
    "# how to get logits from output layer instead of inputs to softmax layer?\n",
    "logits = Y_proba.op.inputs[0]\n",
    "#logits = tf.get_default_graph().get_tensor_by_name(\"dnn/model/bn_outputs:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"dnn/eval/accuracy:0\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"outputs\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "    \n",
    "# eval\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 0 ..., 4 4 2]\n"
     ]
    }
   ],
   "source": [
    "clean_indices = np.where(y_train >= 5)\n",
    "X_train_2_full = X_train[clean_indices] \n",
    "y_train_2_full = y_train[clean_indices] - 5\n",
    "val_clean_indices = np.where(mnist.validation.labels >= 5)\n",
    "X_val_2_full = mnist.validation.images[val_clean_indices]\n",
    "y_val_2_full = mnist.validation.labels[val_clean_indices] - 5\n",
    "print(y_train_2_full)\n",
    "test_clean_indices = np.where(mnist.test.labels >= 5)\n",
    "X_test_2_full = mnist.test.images[test_clean_indices]\n",
    "y_test_2_full = mnist.test.labels[test_clean_indices] - 5\n",
    "def get_n_samples_per_class(X, y, n=100):\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        print(idx)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True ..., False False False]\n",
      "[False False False ..., False False False]\n",
      "[False False False ..., False False  True]\n",
      "[ True False False ..., False False False]\n",
      "[False False False ...,  True  True False]\n",
      "[ True False  True ..., False False False]\n",
      "[False False False ..., False False False]\n",
      "[False False False ...,  True False  True]\n",
      "[False False False ..., False  True False]\n",
      "[False  True False ..., False False False]\n",
      "[False False  True ..., False  True False]\n",
      "[False False False ..., False False  True]\n",
      "[ True False False ..., False False False]\n",
      "[False False False ..., False False False]\n",
      "[False  True False ...,  True False False]\n"
     ]
    }
   ],
   "source": [
    "X_train_2, y_train_2 = get_n_samples_per_class(X_train_2_full, y_train_2_full)\n",
    "X_val_2, y_val_2 = get_n_samples_per_class(X_val_2_full, y_val_2_full, n=30)\n",
    "X_test_2, y_test_2 = get_n_samples_per_class(X_test_2_full, y_test_2_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_ch11_dnn_model\n",
      "0\tValidation loss: 0.761033\tBest loss: 0.761033\tAccuracy: 79.33%\n",
      "1\tValidation loss: 0.543116\tBest loss: 0.543116\tAccuracy: 80.67%\n",
      "2\tValidation loss: 0.481154\tBest loss: 0.481154\tAccuracy: 85.33%\n",
      "3\tValidation loss: 0.484468\tBest loss: 0.481154\tAccuracy: 84.67%\n",
      "4\tValidation loss: 0.493386\tBest loss: 0.481154\tAccuracy: 85.33%\n",
      "5\tValidation loss: 0.445692\tBest loss: 0.445692\tAccuracy: 85.33%\n",
      "6\tValidation loss: 0.387329\tBest loss: 0.387329\tAccuracy: 87.33%\n",
      "7\tValidation loss: 0.384581\tBest loss: 0.384581\tAccuracy: 88.00%\n",
      "8\tValidation loss: 0.377366\tBest loss: 0.377366\tAccuracy: 88.00%\n",
      "9\tValidation loss: 0.380588\tBest loss: 0.377366\tAccuracy: 87.33%\n",
      "10\tValidation loss: 0.439478\tBest loss: 0.377366\tAccuracy: 87.33%\n",
      "11\tValidation loss: 0.380548\tBest loss: 0.377366\tAccuracy: 88.67%\n",
      "12\tValidation loss: 0.407492\tBest loss: 0.377366\tAccuracy: 86.00%\n",
      "13\tValidation loss: 0.370123\tBest loss: 0.370123\tAccuracy: 88.67%\n",
      "14\tValidation loss: 0.374496\tBest loss: 0.370123\tAccuracy: 89.33%\n",
      "15\tValidation loss: 0.372683\tBest loss: 0.370123\tAccuracy: 88.67%\n",
      "16\tValidation loss: 0.377355\tBest loss: 0.370123\tAccuracy: 87.33%\n",
      "17\tValidation loss: 0.353548\tBest loss: 0.353548\tAccuracy: 90.67%\n",
      "18\tValidation loss: 0.374429\tBest loss: 0.353548\tAccuracy: 90.00%\n",
      "19\tValidation loss: 0.350824\tBest loss: 0.350824\tAccuracy: 87.33%\n",
      "20\tValidation loss: 0.384496\tBest loss: 0.350824\tAccuracy: 88.67%\n",
      "21\tValidation loss: 0.360122\tBest loss: 0.350824\tAccuracy: 90.67%\n",
      "22\tValidation loss: 0.390857\tBest loss: 0.350824\tAccuracy: 88.67%\n",
      "23\tValidation loss: 0.368558\tBest loss: 0.350824\tAccuracy: 88.67%\n",
      "24\tValidation loss: 0.389836\tBest loss: 0.350824\tAccuracy: 88.67%\n",
      "25\tValidation loss: 0.369405\tBest loss: 0.350824\tAccuracy: 88.67%\n",
      "26\tValidation loss: 0.383938\tBest loss: 0.350824\tAccuracy: 88.67%\n",
      "27\tValidation loss: 0.376232\tBest loss: 0.350824\tAccuracy: 87.33%\n",
      "28\tValidation loss: 0.394618\tBest loss: 0.350824\tAccuracy: 88.67%\n",
      "29\tValidation loss: 0.375386\tBest loss: 0.350824\tAccuracy: 89.33%\n",
      "30\tValidation loss: 0.373291\tBest loss: 0.350824\tAccuracy: 88.00%\n",
      "31\tValidation loss: 0.372978\tBest loss: 0.350824\tAccuracy: 88.00%\n",
      "32\tValidation loss: 0.435491\tBest loss: 0.350824\tAccuracy: 86.67%\n",
      "33\tValidation loss: 0.388156\tBest loss: 0.350824\tAccuracy: 87.33%\n",
      "34\tValidation loss: 0.370328\tBest loss: 0.350824\tAccuracy: 87.33%\n",
      "35\tValidation loss: 0.409964\tBest loss: 0.350824\tAccuracy: 88.00%\n",
      "36\tValidation loss: 0.408345\tBest loss: 0.350824\tAccuracy: 88.00%\n",
      "37\tValidation loss: 0.377375\tBest loss: 0.350824\tAccuracy: 88.00%\n",
      "38\tValidation loss: 0.399976\tBest loss: 0.350824\tAccuracy: 87.33%\n",
      "39\tValidation loss: 0.386982\tBest loss: 0.350824\tAccuracy: 88.67%\n",
      "Early stopping!\n",
      "Total training time: 2.9s\n",
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 81.00%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, final_model_path)\n",
    "    \n",
    "    for var in output_layer_vars:\n",
    "        var.initializer.run()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        random_idx = np.random.permutation(len(X_train_2))\n",
    "        for random_batch in np.array_split(random_idx, len(X_train_2) // batch_size):\n",
    "            X_batch, y_batch = X_train_2[random_batch], y_train_2[random_batch]\n",
    "            sess.run(training_op, \n",
    "                     feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_val_2, y: y_val_2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = frozen_saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test_2, y: y_test_2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# caching --skip c,d,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n10. Pretraining on an auxiliary task.\\n    a. In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or \\n    not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. Start by building\\n    two DNNs (let’s call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should \\n    have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on \\n    top of both DNNs. To do this, you should use TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs \\n    for each instance, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation \\n    function.\\n\\n    b. Split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. \\n    Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the\\n    training instances should be pairs of images that belong to the same class, while the other half should be images from different classes.\\n    For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes.\\n\\n    c. Train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image\\n    to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not.\\n\\n    d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10\\n    neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class.\\n\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "10. Pretraining on an auxiliary task.\n",
    "    a. In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or \n",
    "    not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. Start by building\n",
    "    two DNNs (let’s call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should \n",
    "    have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on \n",
    "    top of both DNNs. To do this, you should use TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs \n",
    "    for each instance, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation \n",
    "    function.\n",
    "\n",
    "    b. Split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. \n",
    "    Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the\n",
    "    training instances should be pairs of images that belong to the same class, while the other half should be images from different classes.\n",
    "    For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes.\n",
    "\n",
    "    c. Train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image\n",
    "    to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not.\n",
    "\n",
    "    d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10\n",
    "    neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# starter code from Ageron\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier:\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "    \n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "    \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                  logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Make the important operations available easily through instance variables\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "    \n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Translate the labels vector to a vector of sorted class indices, containing\n",
    "        # integers from 0 to n_outputs - 1.\n",
    "        # For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class\n",
    "        # labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector\n",
    "        # will be translated to [3, 3, 4, 0, 2, 1, 1, 1]\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # extra ops for batch normalization\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        # needed in case of early stopping\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        # Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # If we used early stopping then rollback to the best model found\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "def dnn(inputs, name, n_hidden_layers=5, n_neurons=100, activation=tf.nn.elu,\n",
    "        initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    with tf.variable_scope(name, \"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons,\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        name=\"hidden\" + str(layer+1) + \"_\" + name)\n",
    "            inputs = activation(inputs, name=\"hidden\" + str(layer+1) + \"_out\"\n",
    "                                + \"_\" + name)\n",
    "        return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, 2, 784), name=\"X\")\n",
    "X1, X2 = tf.unstack(X, axis=1)\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "dnn_a = dnn(X1, 'a')\n",
    "dnn_b = dnn(X2, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_layer_inputs = tf.concat([dnn_a, dnn_b], 1)\n",
    "top_hidden_layer = dnn(top_layer_inputs, \"top_layer\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = dnn(top_hidden_layer, \"logits\", 1)\n",
    "y_proba = tf.sigmoid(logits)\n",
    "y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_as_float = tf.cast(y, tf.float32)\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.95\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_correct = tf.equal(y_pred, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1, y_train1 = mnist.train.next_batch(mnist.train.num_examples)\n",
    "X_train2, y_train2 = mnist.train.next_batch(5000)\n",
    "\n",
    "def sort_samples_by_class(X, y):\n",
    "    class_indicies = {}\n",
    "    for label in np.unique(y):\n",
    "        if label not in class_indicies:\n",
    "            class_indicies[label] = np.where(y_train == label)\n",
    "    return class_indicies\n",
    "\n",
    "sorted_classes = sort_samples_by_class(X_train1, y_train1)\n",
    "def get_split_batch(batch_size):\n",
    "    #sorted_classes = sort_samples_by_class(X_train1, y_train1)\n",
    "    X = []\n",
    "    y = []\n",
    "    # Make set of matching samples\n",
    "    for i in range(batch_size//2):\n",
    "        random_class = np.random.randint(10)\n",
    "        class_idx = list(sorted_classes[random_class][0])\n",
    "        random_pair = np.random.randint(len(class_idx), size=2)\n",
    "        X.append((X_train[class_idx[random_pair[0]]], X_train[class_idx[random_pair[1]]]))\n",
    "        y.append([0])\n",
    "    # Make set of mismatched samples\n",
    "    for j in range(batch_size//2):\n",
    "        random_class1 = np.random.randint(10)\n",
    "        random_class2 = np.random.choice(list(range(0,random_class1)) + \n",
    "                                        list(range(random_class1+1, 10)))\n",
    "        \n",
    "        class_idx1 = list(sorted_classes[random_class1][0])\n",
    "        random_idx1 = np.random.randint(len(class_idx1))\n",
    "        \n",
    "        class_idx2 = list(sorted_classes[random_class2][0])\n",
    "        random_idx2 = np.random.randint(len(class_idx2))\n",
    "        X.append((X_train[class_idx1[random_idx1]], X_train[class_idx2[random_idx2]]))\n",
    "        y.append([1])\n",
    "        \n",
    "    random_indices = np.random.permutation(batch_size)\n",
    "    return np.array(X)[random_indices], np.array(y)[random_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test1, y_test1 = get_split_batch(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 0.695476\n",
      "0 Test accuracy: 0.51441\n",
      "1 Train loss: 0.612364\n",
      "2 Train loss: 0.436901\n",
      "3 Train loss: 0.380417\n",
      "4 Train loss: 0.363631\n",
      "5 Train loss: 0.352584\n",
      "5 Test accuracy: 0.902954\n",
      "6 Train loss: 0.325875\n",
      "7 Train loss: 0.320604\n",
      "8 Train loss: 0.34232\n",
      "9 Train loss: 0.26354\n",
      "10 Train loss: 0.253097\n",
      "10 Test accuracy: 0.939332\n",
      "11 Train loss: 0.269489\n",
      "12 Train loss: 0.221965\n",
      "13 Train loss: 0.242726\n",
      "14 Train loss: 0.25304\n",
      "15 Train loss: 0.23549\n",
      "15 Test accuracy: 0.958002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-128830e0908a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_split_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-e2a07036bcee>\u001b[0m in \u001b[0;36mget_split_batch\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m                                         list(range(random_class1+1, 10)))\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mclass_idx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_classes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_class1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mrandom_idx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_idx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m#X_diff1.append(X_train[class_idx1[random_idx1]])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(X_train1)//batch_size):\n",
    "            X_batch, y_batch = get_split_batch(batch_size)\n",
    "            loss_val, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        print(epoch, \"Train loss:\", loss_val)\n",
    "        if epoch % 5 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "            \n",
    "    save_path = saver.save(sess, \"./my_digit_comparison_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 784), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X, name=\"a\")\n",
    "frozen_outputs = tf.stop_gradient(dnn_outputs)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, 10, kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dnn_a_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"a\")\n",
    "\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_a_vars})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ...,  True False False]\n",
      "[False False False ..., False False False]\n",
      "[False  True  True ..., False False False]\n",
      "[False False False ..., False False False]\n",
      "[False False False ..., False False False]\n",
      "[ True False False ..., False False False]\n",
      "[False False False ..., False False False]\n",
      "[False False False ..., False False False]\n",
      "[False False False ..., False  True False]\n",
      "[False False False ..., False False  True]\n",
      "(4821, 784)\n",
      "(?, 784)\n",
      "99 Test accuracy: 0.9476\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "X_train3, y_train3 = get_n_samples_per_class(X_train2, y_train2, n=500)\n",
    "print(X_train3.shape)\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(X.shape)\n",
    "    for epoch in range(n_epochs):\n",
    "        random_idx = np.random.permutation(len(X_train3))\n",
    "        for batch in np.array_split(random_idx, len(X_train3) // batch_size):\n",
    "            X_batch, y_batch = X_train2[batch], y_train2[batch]\n",
    "            #print(batch)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "        print(epoch, \"Test accuracy:\", acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
