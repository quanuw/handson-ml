{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3) that computes A ⊕ B (where ⊕ represents the XOR operation). Hint: A ⊕ B = (A ∧ ¬ B) ∨ (¬ A ∧ B).\\nx1,x2 --> A(detect 2 1s)----\\\\-1\\n                             C(output) \\nx1,x2 --> B(detect 1s)------/+1\\nhttps://stackoverflow.com/questions/6495603/how-to-solve-xor-problem-with-mlp-neural-network\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3) that computes A ⊕ B (where ⊕ represents the XOR operation). Hint: A ⊕ B = (A ∧ ¬ B) ∨ (¬ A ∧ B).\n",
    "x1,x2 --> A(detect 2 1s)----\\-1\n",
    "                             C(output) \n",
    "x1,x2 --> B(detect 1s)------/+1\n",
    "https://stackoverflow.com/questions/6495603/how-to-solve-xor-problem-with-mlp-neural-network\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2. Why is it generally preferable to use a Logistic Regression classifier rather than\\na classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)?\\nHow can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\\nPerceptrons use a step function as its activation function. This means they make class predictions based on\\na hard threshold. While logistic regression can output class probabilities. You can tweak\\na perceptron by adding more layers(MLP) and changing its activation function (logit, tanh, relu).\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2. Why is it generally preferable to use a Logistic Regression classifier rather than\n",
    "a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)?\n",
    "How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
    "Perceptrons use a step function as its activation function. This means they make class predictions based on\n",
    "a hard threshold. While logistic regression can output class probabilities. You can tweak\n",
    "a perceptron by adding more layers(MLP) and changing its activation function (logit, tanh, relu).\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n3. Why was the logistic activation function a key ingredient in training the first MLPs?\\nThe logistic activiation function allowed for gradient descent to work.\\nThe original step function had flat segments which meant gradient descent could not\\nprogress during training.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "3. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "The logistic activiation function allowed for gradient descent to work.\n",
    "The original step function had flat segments which meant gradient descent could not\n",
    "progress during training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4. Name three popular activation functions. Can you draw them?\\nThree popular activation functions are: logit, tanh, ad relu.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4. Name three popular activation functions. Can you draw them?\n",
    "Three popular activation functions are: logit, tanh, ad relu.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\\n\\nWhat is the shape of the input matrix X?\\nThe shape of X is (m, 10). Where m = training batch size.\\nWhat about the shape of the hidden layer’s weight vector Wh, and the shape of its bias vector bh?\\nThe shape of Wh is (10, 50) and bh is (None, 50).\\nWhat is the shape of the output layer’s weight vector Wo, and its bias vector bo?\\nWo will have a shape of (50, 3) and bo will have a shape of (None, 3).\\nWhat is the shape of the network’s output matrix Y?\\nThe shape of Y is (m, 3).\\nWrite the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo and bo.\\nY = ReLU(ReLU(X · Wh + bh) · Wo + bo). Recall that the ReLU function just sets every negative number in the matrix to zero.\\nAlso note that when you are adding a bias vector to a matrix, it is added to every single row in the matrix, which is called broadcasting.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "\n",
    "What is the shape of the input matrix X?\n",
    "The shape of X is (m, 10). Where m = training batch size.\n",
    "What about the shape of the hidden layer’s weight vector Wh, and the shape of its bias vector bh?\n",
    "The shape of Wh is (10, 50) and bh is (None, 50).\n",
    "What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    "Wo will have a shape of (50, 3) and bo will have a shape of (None, 3).\n",
    "What is the shape of the network’s output matrix Y?\n",
    "The shape of Y is (m, 3).\n",
    "Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo and bo.\n",
    "Y = ReLU(ReLU(X · Wh + bh) · Wo + bo). Recall that the ReLU function just sets every negative number in the matrix to zero.\n",
    "Also note that when you are adding a bias vector to a matrix, it is added to every single row in the matrix, which is called broadcasting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n6. How many neurons do you need in the output layer if you want to classify email into spam or ham?\\nWhat activation function should you use in the output layer? \\nIf instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function? \\nAnswer the same questions for getting your network to predict housing prices as in Chapter 2.\\nTo classify email as spam or ham we can just use a single neuron at the output layer and\\nthe step function. If you want a probability then use the logistic function.\\nFor MNIST, the ouput layer will have 10 neurons and\\nthe activiation function used should be softmax which can handle multiple classes and\\noutputs one probability per class.\\nTo predict house prices like in Chapter 3 you'd need one output neuron and no activation function(linear) since\\nit's a regression task and outputs aren't bounded.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "6. How many neurons do you need in the output layer if you want to classify email into spam or ham?\n",
    "What activation function should you use in the output layer? \n",
    "If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function? \n",
    "Answer the same questions for getting your network to predict housing prices as in Chapter 2.\n",
    "To classify email as spam or ham we can just use a single neuron at the output layer and\n",
    "the step function. If you want a probability then use the logistic function.\n",
    "For MNIST, the ouput layer will have 10 neurons and\n",
    "the activiation function used should be softmax which can handle multiple classes and\n",
    "outputs one probability per class.\n",
    "To predict house prices like in Chapter 3 you'd need one output neuron and no activation function(linear) since\n",
    "it's a regression task and outputs aren't bounded.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n7. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\\nBackpropagation is: 'Let’s make this even shorter: for each training instance the backpropagation algorithm first makes a prediction (forward pass),\\nmeasures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), \\nand finally slightly tweaks the connection weights to reduce the error (Gradient Descent step).' Reverse-mode autodiff is used to \\ncompute the gradients for the GD step which is used by the backpropagation algo.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "7. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "Backpropagation is: 'Let’s make this even shorter: for each training instance the backpropagation algorithm first makes a prediction (forward pass),\n",
    "measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), \n",
    "and finally slightly tweaks the connection weights to reduce the error (Gradient Descent step).' Reverse-mode autodiff is used to \n",
    "compute the gradients for the GD step which is used by the backpropagation algo.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n8. Can you list all the hyperparameters you can tweak in an MLP? \\nIf the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\\nThe hyperparameters are: the number of hidden layers, the number of neurons in each hidden layer, \\nand the activation function used in each hidden layer and in the output layer. In general, the ReLU activation function (or one of its variants; see Chapter 11)\\nis a good default for the hidden layers. For the output layer, in general you will want the logistic activation function for binary classification,\\nthe softmax activation function for multiclass classification, or no activation function for regression.\\nIf the MLP is overfitting the data then you can try to reduce the number of hidden layers and\\nreduce the number of neurons per hidden layer.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "8. Can you list all the hyperparameters you can tweak in an MLP? \n",
    "If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "The hyperparameters are: the number of hidden layers, the number of neurons in each hidden layer, \n",
    "and the activation function used in each hidden layer and in the output layer. In general, the ReLU activation function (or one of its variants; see Chapter 11)\n",
    "is a good default for the hidden layers. For the output layer, in general you will want the logistic activation function for binary classification,\n",
    "the softmax activation function for multiclass classification, or no activation function for regression.\n",
    "If the MLP is overfitting the data then you can try to reduce the number of hidden layers and\n",
    "reduce the number of neurons per hidden layer.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.94 Val accuracy: 0.8794\n",
      "1 Train accuracy: 0.92 Val accuracy: 0.9044\n",
      "2 Train accuracy: 0.92 Val accuracy: 0.9158\n",
      "3 Train accuracy: 0.88 Val accuracy: 0.923\n",
      "4 Train accuracy: 0.96 Val accuracy: 0.9312\n",
      "5 Train accuracy: 0.92 Val accuracy: 0.9352\n",
      "6 Train accuracy: 0.96 Val accuracy: 0.942\n",
      "7 Train accuracy: 0.92 Val accuracy: 0.9448\n",
      "8 Train accuracy: 0.98 Val accuracy: 0.948\n",
      "9 Train accuracy: 0.96 Val accuracy: 0.9506\n",
      "10 Train accuracy: 0.96 Val accuracy: 0.9542\n",
      "11 Train accuracy: 0.96 Val accuracy: 0.9572\n",
      "12 Train accuracy: 0.98 Val accuracy: 0.9586\n",
      "13 Train accuracy: 0.96 Val accuracy: 0.9604\n",
      "14 Train accuracy: 1.0 Val accuracy: 0.9612\n",
      "15 Train accuracy: 0.96 Val accuracy: 0.9628\n",
      "16 Train accuracy: 0.98 Val accuracy: 0.9636\n",
      "17 Train accuracy: 0.98 Val accuracy: 0.964\n",
      "18 Train accuracy: 1.0 Val accuracy: 0.9658\n",
      "19 Train accuracy: 0.98 Val accuracy: 0.9672\n",
      "20 Train accuracy: 0.96 Val accuracy: 0.9664\n",
      "21 Train accuracy: 1.0 Val accuracy: 0.9686\n",
      "22 Train accuracy: 0.98 Val accuracy: 0.9674\n",
      "23 Train accuracy: 0.96 Val accuracy: 0.9674\n",
      "24 Train accuracy: 0.96 Val accuracy: 0.97\n",
      "25 Train accuracy: 0.96 Val accuracy: 0.9694\n",
      "26 Train accuracy: 1.0 Val accuracy: 0.9712\n",
      "27 Train accuracy: 0.98 Val accuracy: 0.9722\n",
      "28 Train accuracy: 1.0 Val accuracy: 0.972\n",
      "29 Train accuracy: 1.0 Val accuracy: 0.971\n",
      "30 Train accuracy: 0.98 Val accuracy: 0.972\n",
      "31 Train accuracy: 0.98 Val accuracy: 0.971\n",
      "32 Train accuracy: 0.98 Val accuracy: 0.9732\n",
      "33 Train accuracy: 1.0 Val accuracy: 0.9732\n",
      "34 Train accuracy: 0.98 Val accuracy: 0.9724\n",
      "35 Train accuracy: 1.0 Val accuracy: 0.974\n",
      "36 Train accuracy: 0.96 Val accuracy: 0.9734\n",
      "37 Train accuracy: 0.98 Val accuracy: 0.9734\n",
      "38 Train accuracy: 1.0 Val accuracy: 0.975\n",
      "39 Train accuracy: 0.98 Val accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "9. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision.\n",
    "Just like in the last exercise of Chapter 9, try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, add summaries, plot learning curves using TensorBoard, and so on).\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 30\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "        if acc_val > 0.98:\n",
    "            print('Val accuracy > 98%')\n",
    "            break\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def dnn(X, y, initializer=None, learning_rate=0.005):\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "            hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "            hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\",\n",
    "                              activation=tf.nn.relu)\n",
    "            logits = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return accuracy, training_op, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training was interrupted. Continuing at epoch 341\n",
      "INFO:tensorflow:Restoring parameters from /tmp/my_dnn_model.ckpt\n",
      "Test accuracy: 0.9806\n"
     ]
    }
   ],
   "source": [
    "logdir = log_dir(\"dnn\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 784), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "accuracy, training_op, init, saver = dnn(X, y)\n",
    "\n",
    "import os\n",
    "n_epochs = 151\n",
    "batch_size = 50\n",
    "\n",
    "checkpoint_path = \"./tmp/my_dnn_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_dnn_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        #loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "        #file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tAcc:\", acc_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    acc_test_val = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\"Test accuracy:\", acc_test_val)\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
