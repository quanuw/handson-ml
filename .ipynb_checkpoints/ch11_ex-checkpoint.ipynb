{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\\nNo, doing this will prevent the network from breaking symmetry. Each unit gets a signal\\nequal to the sum of inputs and outputs sigmoid(sum(inputs)). If all weights are zero then\\nall hidden units will get zero signal. So if all weights are the same, then all units\\nin the hidden layer will be the same too no matter the input. Simply, if all\\nunits start with the same wieght, then they will follow the same gradient which could severely\\nslow down learning if there are many local minima.\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "No, doing this will prevent the network from breaking symmetry. Each unit gets a signal\n",
    "equal to the sum of inputs and outputs sigmoid(sum(inputs)). If all weights are zero then\n",
    "all hidden units will get zero signal. So if all weights are the same, then all units\n",
    "in the hidden layer will be the same too no matter the input. Simply, if all\n",
    "units start with the same wieght, then they will follow the same gradient which could severely\n",
    "slow down learning if there are many local minima.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2. Is it okay to initialize the bias terms to 0?\\nYes, because symmetry breaking is done by initializing weights as random values.\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2. Is it okay to initialize the bias terms to 0?\n",
    "Yes, because symmetry breaking is done by initializing weights as random values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n3. Name three advantages of the ELU activation function over ReLU.\\n    a. ELU won't saturate (gradient = 0) when z < 0, so units won't die\\n    b. The function is smooth everywhere including z = 0 which helps gradient\\n    descent because it does not bounce as much lect and right of z=0.\\n    c. ELU takes on negative values when z < 0, which allows the unit to have an average output closer to 0. \\n    This helps alleviate the vanishing gradients problem, as discussed earlier.\\n\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "3. Name three advantages of the ELU activation function over ReLU.\n",
    "    a. ELU won't saturate (gradient = 0) when z < 0, so units won't die\n",
    "    b. The function is smooth everywhere including z = 0 which helps gradient\n",
    "    descent because it does not bounce as much lect and right of z=0.\n",
    "    c. ELU takes on negative values when z < 0, which allows the unit to have an average output closer to 0. \n",
    "    This helps alleviate the vanishing gradients problem, as discussed earlier.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4. In which cases would you want to use each of the following activation functions: \\nELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\\nELU: Generally the best to use, but slower to compute than ReLU and its variants.\\nleaky ReLU: Use if you care a lot about runtime performance. Preferred over ReLU\\non large datasets, but might overfit on smaller datasets.\\nReLU: More simple than leaky ReLU, but in general leaky and ELU are better. ReLU\\nis able to output zero in some cases which can be useful.\\ntanh: Quicker convergence then logstic functions. \\nCan also be useful in the output layer if you need to output a number between –1 and 1, \\nbut nowadays it is not used much in hidden layers. \\nlogistic: Useful if you want to estimate a probability, but is rarely used in hidden layers.\\nsoftmax: Useful in output layer to output mulually exclusive classes (MNIST labels).\\n\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4. In which cases would you want to use each of the following activation functions: \n",
    "ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "ELU: Generally the best to use, but slower to compute than ReLU and its variants.\n",
    "leaky ReLU: Use if you care a lot about runtime performance. Preferred over ReLU\n",
    "on large datasets, but might overfit on smaller datasets.\n",
    "ReLU: More simple than leaky ReLU, but in general leaky and ELU are better. ReLU\n",
    "is able to output zero in some cases which can be useful.\n",
    "tanh: Quicker convergence then logstic functions. \n",
    "Can also be useful in the output layer if you need to output a number between –1 and 1, \n",
    "but nowadays it is not used much in hidden layers. \n",
    "logistic: Useful if you want to estimate a probability, but is rarely used in hidden layers.\n",
    "softmax: Useful in output layer to output mulually exclusive classes (MNIST labels).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\\nIf the momentum hyperparameter is too close to 1 then it will make the optimizer\\noscillate (overshoot a bit, then come back, overshoot again) many times before stablizing at the minimum.\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
    "If the momentum hyperparameter is too close to 1 then it will make the optimizer\n",
    "oscillate (overshoot a bit, then come back, overshoot again) many times before stablizing at the minimum.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n6. Name three ways you can produce a sparse model.\\n    a. Train the model as usual, then get rid of the tiny weights (set them to 0).\\n    b. Use ℓ1 regularization during training, which pushes the optimizer toward sparsity\\n    c. Apply Dual Averaging, often called Follow The Regularized Leader (FTRL). When used with ℓ1 regularization, this technique often leads to very sparse models. \\n    TensorFlow implements a variant of FTRL called FTRL-Proximal18 in the FTRLOptimizer class.\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "6. Name three ways you can produce a sparse model.\n",
    "    a. Train the model as usual, then get rid of the tiny weights (set them to 0).\n",
    "    b. Use ℓ1 regularization during training, which pushes the optimizer toward sparsity\n",
    "    c. Apply Dual Averaging, often called Follow The Regularized Leader (FTRL). When used with ℓ1 regularization, this technique often leads to very sparse models. \n",
    "    TensorFlow implements a variant of FTRL called FTRL-Proximal18 in the FTRLOptimizer class.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\\nYes, dropout tends to slow down convergence, but usually results in a better model. \\nNo, it does't slow down inference because dropout is only done during training.\\n\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
    "Yes, dropout tends to slow down convergence, but usually results in a better model. \n",
    "No, it does't slow down inference because dropout is only done during training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n8. Deep Learning.\\n\\n    a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\\n\\n    b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\\n\\n    c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\\n\\n    d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\\n\\n    e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "8. Deep Learning.\n",
    "\n",
    "    a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "\n",
    "    b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "\n",
    "    c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "\n",
    "    d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n",
    "\n",
    "    e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "55000\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "print(mnist.train.num_examples)\n",
    "\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 100\n",
    "n_hidden5 = 100\n",
    "n_outputs = 5\n",
    "\n",
    "\n",
    "def dnn(X, y, initializer=None, learning_rate=0.001):\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,)\n",
    "            hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,)\n",
    "            hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\",\n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,)\n",
    "            hidden4 = tf.layers.dense(hidden3, n_hidden4, name=\"hidden4\",\n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,)\n",
    "            hidden5 = tf.layers.dense(hidden4, n_hidden5, name=\"hidden5\",\n",
    "                        activation=tf.nn.elu, kernel_initializer=he_init,)\n",
    "            logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return accuracy, training_op, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir = log_dir(\"dnn\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 784), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "accuracy, training_op, init, saver = dnn(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28038\n",
      "0 Train accuracy: 1.0 Val accuracy: 0.9859265\n",
      "Epoch: 0 \tAcc: 0.9859265\n",
      "1 Train accuracy: 1.0 Val accuracy: 0.98319\n",
      "2 Train accuracy: 1.0 Val accuracy: 0.98670834\n",
      "3 Train accuracy: 1.0 Val accuracy: 0.9859265\n",
      "4 Train accuracy: 1.0 Val accuracy: 0.99022675\n",
      "5 Train accuracy: 1.0 Val accuracy: 0.99296325\n",
      "Epoch: 5 \tAcc: 0.99296325\n",
      "6 Train accuracy: 0.98 Val accuracy: 0.9917905\n",
      "7 Train accuracy: 1.0 Val accuracy: 0.9910086\n",
      "8 Train accuracy: 1.0 Val accuracy: 0.9898358\n",
      "9 Train accuracy: 1.0 Val accuracy: 0.9925723\n",
      "10 Train accuracy: 1.0 Val accuracy: 0.9925723\n",
      "Epoch: 10 \tAcc: 0.9925723\n",
      "11 Train accuracy: 1.0 Val accuracy: 0.9906177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-a8d8b7f138ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m#loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0macc_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "n_epochs = 151\n",
    "batch_size = 50\n",
    "\n",
    "checkpoint_path = \"/tmp/my_ch11_dnn_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_ch11_dnn_model\"\n",
    "\n",
    "acc_best = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        \n",
    "    # get training samples where y < 5\n",
    "    X_train, y_train = mnist.train.next_batch(mnist.train.num_examples)\n",
    "    clean_indices = np.where(y_train < 5)\n",
    "    X_train_clean = X_train[clean_indices]\n",
    "    y_train_clean = y_train[clean_indices]\n",
    "    print(len(X_train_clean))\n",
    "    \n",
    "    # get validation samples where y < 5\n",
    "    val_clean_indices = np.where(mnist.validation.labels < 5)\n",
    "    X_val_clean = mnist.validation.images[val_clean_indices]\n",
    "    y_val_clean = mnist.validation.labels[val_clean_indices]\n",
    "    \n",
    "    # get test samples where y < 5\n",
    "    test_clean_indices = np.where(mnist.test.labels < 5)\n",
    "    X_test_clean = mnist.test.images[test_clean_indices]\n",
    "    y_test_clean = mnist.test.labels[test_clean_indices]\n",
    "    \n",
    "    def next_batch(batch_size):\n",
    "        random = np.random.choice(len(X_train_clean), batch_size)\n",
    "        return X_train_clean[random], y_train_clean[random]\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        #loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_val_clean,\n",
    "                                           y: y_val_clean})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "        #file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tAcc:\", acc_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            if acc_val > acc_best:\n",
    "                acc_best = acc_val\n",
    "                saver.save(sess, final_model_path)\n",
    "            else:\n",
    "                break\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "                \n",
    "# restore best model\n",
    "with tf.Session() as sess:\n",
    "    print('restoring the best model')\n",
    "    saver.restore(sess, final_model_path)\n",
    "    acc_test_val = accuracy.eval(feed_dict={X: X_test_clean, y: y_test_clean})\n",
    "    print(\"Test accuracy:\", acc_test_val)\n",
    "\n",
    "os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
