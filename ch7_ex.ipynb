{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\\nNo, because they are trained on the same data and will have correlated errors and won't be perfectly independent.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "No, because they are trained on the same data and will have correlated errors and won't be perfectly independent.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2. What is the difference between hard and soft voting classifiers?\\nHard voting classifiers aggregate the predcitions of each classifier and predicts the class\\nthat gets the most votes. Soft voting classifiers predict the class with the\\nhighest class probability, averaged over all the individual classifiers.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2. What is the difference between hard and soft voting classifiers?\n",
    "Hard voting classifiers aggregate the predcitions of each classifier and predicts the class\n",
    "that gets the most votes. Soft voting classifiers predict the class with the\n",
    "highest class probability, averaged over all the individual classifiers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\\nYes, bagging, pasting, and random forest ensembles can be trained in parallel. Boosting and stacking ensembles can't be trained in\\nparallel because they depend on previous predictors.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
    "Yes, bagging, pasting, and random forest ensembles can be trained in parallel. Boosting and stacking ensembles can't be trained in\n",
    "parallel because they depend on previous predictors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4. What is the benefit of out-of-bag evaluation?\\nOOB evalution allows the user to use instances from the training set as test instances.\\nThis means there is no need for a seperate validation set or cross validation.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4. What is the benefit of out-of-bag evaluation?\n",
    "OOB evalution allows the user to use instances from the training set as test instances.\n",
    "This means there is no need for a seperate validation set or cross validation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?\\nExtra-Trees are more random because they use a random  splitting threshold for each feature rather than\\nsearching for the best possibile thresholds. This extra randomness can help by\\nlowering variance (decorrelation of trees) but also increases bias (less optimal splits).\\nExtra-Trees are faster than regular Random Forests becuase finding the best possible \\nthreshold for each feature at every node is he most time-consuming task of growing a tree.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?\n",
    "Extra-Trees are more random because they use a random  splitting threshold for each feature rather than\n",
    "searching for the best possibile thresholds. This extra randomness can help by\n",
    "lowering variance (decorrelation of trees) but also increases bias (less optimal splits).\n",
    "Extra-Trees are faster than regular Random Forests becuase finding the best possible \n",
    "threshold for each feature at every node is he most time-consuming task of growing a tree.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\\nAdjust the learning rate and/or number of estimators.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\n",
    "Adjust the learning rate and/or number of estimators.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\\nIf Gradient Boosting is overfitting then lower the learning rate.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
    "If Gradient Boosting is overfitting then lower the learning rate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "shuffle = np.random.permutation(70000)\n",
    "X = X[shuffle]\n",
    "y = y[shuffle]\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = X[:40000], X[40000:50000], X[50000:60000], y[:40000], y[40000:50000], y[50000:60000]\n",
    "\n",
    "shuffle_index = np.random.permutation(40000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K nearest neighbor:  0.9673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_knn.fit(X_train, y_train)\n",
    "y_pred_knn = clf_knn.predict(X_val)\n",
    "acc_knn = accuracy_score(y_val, y_pred_knn)\n",
    "print(\"K nearest neighbor: \",acc_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest accuracy:  0.938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_pred_rf = clf_rf.predict(X_val)\n",
    "acc_rf = accuracy_score(y_val, y_pred_rf)\n",
    "print(\"random forest accuracy: \",acc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra tree accuracy:  0.9431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf_et = ExtraTreesClassifier()\n",
    "clf_et.fit(X_train, y_train)\n",
    "y_pred_et = clf_et.predict(X_val)\n",
    "acc_et = accuracy_score(y_val, y_pred_et)\n",
    "print(\"extra tree accuracy: \",acc_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting accuracy:  0.9708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('knn', clf_knn), ('rf', clf_rf), ('et', clf_et)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred_voting = voting_clf.predict(X_val)\n",
    "acc_voting = accuracy_score(y_val, y_pred_voting)\n",
    "print(\"voting accuracy: \",acc_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = voting_clf.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "print(\"voting test set accuracy: \",acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 9\n",
    "estimators = [clf_knn, clf_rf, clf_et]\n",
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "print(X_val_predictions)\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_val_predictions[:, index] = estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "rnd_forest_blender.fit(X_val_predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd_forest_blender.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tes_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = rnd_forest_blender.predict(X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_blender = accuracy_score(y_test, y_pred)\n",
    "print(\"blender test accuracy: \", acc_blender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
