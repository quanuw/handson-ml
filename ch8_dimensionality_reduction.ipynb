{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?\n",
    "The general motivations for reducing a dataset's dimensionality are: to speed up training,\n",
    "increase performance, and data visiaulization. Some drawbacks include: loss of information and \n",
    "increased pipeline complexity.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. What is the curse of dimensionality?\n",
    "The curse of dimensionality is that the more dimensions a training set has,\n",
    "the greater the risk of overfitting it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?\n",
    "It is possible to reverse the operation using the inverse transformation on the reduced\n",
    "training set. Although, reversing the operation is possible, the data of the reversed\n",
    "training set will decrease in quality due to the information lost in the prior reduction.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n",
    "No, PCA should not be used to reduce a hightly nonlinear dataset because PCA\n",
    "is a linear reduction method.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?\n",
    "Resulting dataset will have however many dimension is needed to have an explained\n",
    "variance ratio of 95%.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?\n",
    "Vanilla PCA: Use when training set is highly linear and not too large.\n",
    "\n",
    "Incremental PCA: Use when you want to train in batches (online training) or when the\n",
    "training set is very large.\n",
    "\n",
    "Randomized PCA: Use when d is much smaller than n. Will be dramatically faster\n",
    "than vanilla PCA.\n",
    "\n",
    "Kernal PCA: Use when the datset is nonlinear.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?\n",
    "You can use grid search on a supervised learning task after your dataset is reduced to\n",
    "evaluate performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "8. Does it make any sense to chain two different dimensionality reduction algorithms?\n",
    "No.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "9. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). \n",
    "Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset’s dimensionality, with an explained variance ratio of 95%. \n",
    "Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next evaluate the classifier on the test set: how does it compare to the previous classifier?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "shuffle = np.random.permutation(70000)\n",
    "X = X[shuffle]\n",
    "y = y[shuffle]\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "clf_rf = RandomForestClassifier()\n",
    "t0 = time()\n",
    "clf_rf.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred)\n",
    "print(\"random forest accuracy: \",acc_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_train, X_test = X_reduced[:60000], X_reduced[60000:]\n",
    "clf_rf = RandomForestClassifier()\n",
    "t0 = time()\n",
    "clf_rf.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred)\n",
    "print(\"random forest accuracy: \",acc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "10. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. \n",
    "You can use a scatterplot using 10 different colors to represent each image’s target class. \n",
    "Alternatively, you can write colored digits at the location of each instance, or even plot scaled-down versions\n",
    "of the digit images themselves (if you plot all digits, the visualization will be too cluttered, \n",
    "so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). \n",
    "You should get a nice visualization with well-separated clusters of digits. \n",
    "Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "import matplotlib.pyplot as plt\n",
    "tsne = manifold.TSNE(n_components=2)\n",
    "X_reduced = tsne.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=\"jet\")\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "cmap = matplotlib.cm.get_cmap(\"jet\")\n",
    "for digit in (2, 3, 5):\n",
    "    plt.scatter(X_reduced[y == digit, 0], X_reduced[y == digit, 1], c=cmap(digit / 9))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
